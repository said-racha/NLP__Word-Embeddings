{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NLP & representation learning: Neural Embeddings, Text Classification\n",
                "\n",
                "\n",
                "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **Bag of Word (BoW)** model. \n",
                "\n",
                "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
                "\n",
                "## [Dataset](https://thome.isir.upmc.fr/classes/RITAL/json_pol.json)\n",
                "\n",
                "\n",
                "## \"Modern\" NLP pipeline\n",
                "\n",
                "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
                "\n",
                "\n",
                "The raw classification pipeline is then the following:\n",
                "\n",
                "```\n",
                "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
                "```\n",
                "\n",
                "\n",
                "### Using a  language model:\n",
                "\n",
                "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
                "\n",
                "In this setting, the pipeline becomes the following:\n",
                "```\n",
                "      \n",
                "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
                "```\n",
                "\n",
                "\n",
                "- #### Classic word embeddings\n",
                "\n",
                " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
                " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
                "\n",
                "\n",
                "- #### bleeding edge language models techniques (see next)\n",
                "\n",
                " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
                " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
                " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
                " - [BERT](https://arxiv.org/abs/1810.04805)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "### Goal of this session:\n",
                "\n",
                "1. Train word embeddings on training dataset\n",
                "2. Tinker with the learnt embeddings and see learnt relations\n",
                "3. Tinker with pre-trained embeddings.\n",
                "4. Use those embeddings for classification\n",
                "5. Compare different embedding models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score\n",
                "import pandas as pd\n",
                "import warnings\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 135,
            "metadata": {},
            "outputs": [],
            "source": [
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 0: Loading data "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of reviews :  25000\n",
                        "----> # of positive :  12500\n",
                        "----> # of negative :  12500\n",
                        "\n",
                        "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "from collections import Counter\n",
                "\n",
                "# Loading json\n",
                "file = './datasets/json_pol.json'\n",
                "with open(file,encoding=\"utf-8\") as f:\n",
                "    data = json.load(f)\n",
                "    \n",
                "\n",
                "# Quick Check\n",
                "counter = Counter((x[1] for x in data))\n",
                "print(\"Number of reviews : \", len(data))\n",
                "print(\"----> # of positive : \", counter[1])\n",
                "print(\"----> # of negative : \", counter[0])\n",
                "print(\"\")\n",
                "print(data[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Word2Vec: Quick Recap\n",
                "\n",
                "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
                "\n",
                "\n",
                "given a random text: `i'm taking the dog out for a walk`\n",
                "\n",
                "\n",
                "\n",
                "### (a) Continuous Bag of Word (CBOW)\n",
                "    -  predicts a word given a context\n",
                "    \n",
                "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
                "    \n",
                "### (b) Skip-Gram (SG)               \n",
                "    -  predicts a context given a word\n",
                "    \n",
                " maximizing `p(i'm taking the out for a walk | dog)`\n",
                "\n",
                "\n",
                "\n",
                "   "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 1: train a language model (word2vec)\n",
                "\n",
                "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
                "\n",
                "\n",
                "### Train:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# if gensim not installed yet\n",
                "# ! pip install gensim"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-02-29 11:42:28,338 : INFO : collecting all words and their counts\n",
                        "2024-02-29 11:42:28,338 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
                        "2024-02-29 11:42:29,082 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
                        "2024-02-29 11:42:29,731 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
                        "2024-02-29 11:42:30,020 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
                        "2024-02-29 11:42:30,021 : INFO : Creating a fresh vocabulary\n",
                        "2024-02-29 11:42:30,263 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-29T11:42:30.263575', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
                        "2024-02-29 11:42:30,264 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-29T11:42:30.264830', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
                        "2024-02-29 11:42:30,539 : INFO : deleting the raw counts dictionary of 276678 items\n",
                        "2024-02-29 11:42:30,552 : INFO : sample=0.001 downsamples 44 most-common words\n",
                        "2024-02-29 11:42:30,555 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-29T11:42:30.555092', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
                        "2024-02-29 11:42:30,972 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
                        "2024-02-29 11:42:30,973 : INFO : resetting layer weights\n",
                        "2024-02-29 11:42:31,012 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-29T11:42:31.012168', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
                        "2024-02-29 11:42:31,013 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-29T11:42:31.013184', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
                        "2024-02-29 11:42:32,028 : INFO : EPOCH 0 - PROGRESS: at 23.61% examples, 979485 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:33,033 : INFO : EPOCH 0 - PROGRESS: at 50.49% examples, 1043288 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:34,041 : INFO : EPOCH 0 - PROGRESS: at 77.67% examples, 1066963 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-29 11:42:34,910 : INFO : EPOCH 0: training on 5713167 raw words (4164128 effective words) took 3.9s, 1070652 effective words/s\n",
                        "2024-02-29 11:42:35,918 : INFO : EPOCH 1 - PROGRESS: at 26.91% examples, 1119367 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:36,912 : INFO : EPOCH 1 - PROGRESS: at 51.76% examples, 1073004 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-29 11:42:37,913 : INFO : EPOCH 1 - PROGRESS: at 74.36% examples, 1029159 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-29 11:42:38,816 : INFO : EPOCH 1: training on 5713167 raw words (4165637 effective words) took 3.9s, 1067176 effective words/s\n",
                        "2024-02-29 11:42:39,823 : INFO : EPOCH 2 - PROGRESS: at 28.08% examples, 1168517 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:40,821 : INFO : EPOCH 2 - PROGRESS: at 52.65% examples, 1090269 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:41,820 : INFO : EPOCH 2 - PROGRESS: at 78.90% examples, 1088459 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:42,653 : INFO : EPOCH 2: training on 5713167 raw words (4163995 effective words) took 3.8s, 1084599 effective words/s\n",
                        "2024-02-29 11:42:43,672 : INFO : EPOCH 3 - PROGRESS: at 26.58% examples, 1100169 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:44,671 : INFO : EPOCH 3 - PROGRESS: at 52.65% examples, 1084886 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:45,671 : INFO : EPOCH 3 - PROGRESS: at 80.14% examples, 1102260 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:46,389 : INFO : EPOCH 3: training on 5713167 raw words (4163823 effective words) took 3.7s, 1117491 effective words/s\n",
                        "2024-02-29 11:42:47,398 : INFO : EPOCH 4 - PROGRESS: at 27.17% examples, 1127815 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-29 11:42:48,400 : INFO : EPOCH 4 - PROGRESS: at 52.98% examples, 1095766 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-29 11:42:49,410 : INFO : EPOCH 4 - PROGRESS: at 80.99% examples, 1114175 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-29 11:42:50,077 : INFO : EPOCH 4: training on 5713167 raw words (4164610 effective words) took 3.7s, 1131603 effective words/s\n",
                        "2024-02-29 11:42:50,078 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20822193 effective words) took 19.1s, 1092182 effective words/s', 'datetime': '2024-02-29T11:42:50.078931', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
                        "2024-02-29 11:42:50,079 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-29T11:42:50.079962', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n"
                    ]
                }
            ],
            "source": [
                "import gensim\n",
                "import logging\n",
                "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
                "\n",
                "text = [t.split() for t,p in data]\n",
                "\n",
                "# the following configuration is the default configuration\n",
                "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
                "                                vector_size=100, window=5,               ### here we train a cbow model \n",
                "                                min_count=5,                      \n",
                "                                sample=0.001, workers=3,\n",
                "                                sg=0, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
                "                                cbow_mean=1, epochs=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-02-25 17:32:17,942 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'W2v-movies.dat', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-02-25T17:32:17.942126', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'saving'}\n",
                        "2024-02-25 17:32:17,943 : INFO : not storing attribute cum_table\n",
                        "2024-02-25 17:32:18,031 : INFO : saved W2v-movies.dat\n"
                    ]
                }
            ],
            "source": [
                "# Worth it to save the previous embedding\n",
                "w2v.save(\"W2v-movies.dat\")\n",
                "# You will be able to reload them:\n",
                "#w2v = gensim.models.Word2Vec.load(\"W2v-movies.dat\")\n",
                "# and you can continue the learning process if needed"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 2: Test learnt embeddings\n",
                "\n",
                "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
                "\n",
                "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "great and good: 0.77909327\n",
                        "great and bad: 0.52971756\n"
                    ]
                }
            ],
            "source": [
                "# cosine similarity\n",
                "# is great really closer to good than to bad ?\n",
                "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
                "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[('walk', 0.8287126421928406), ('fly', 0.7930590510368347), ('hang', 0.7786651253700256), ('drive', 0.7679545283317566), ('jump', 0.7633776664733887)]\n",
                        "[('sell', 0.8768139481544495), ('push', 0.8391414880752563), ('blow', 0.824489414691925), ('join', 0.8211827874183655), ('drive', 0.8069530725479126)]\n"
                    ]
                }
            ],
            "source": [
                "# The query can be as simple as a word, such as \"movie\"\n",
                "\n",
                "# Try changing the word\n",
                "#w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
                "#w2v.wv.most_similar(\"awesome\",topn=5)\n",
                "print(w2v.wv.most_similar(\"actor\",topn=5))\n",
                "print(w2v.wv.most_similar(\"run\",topn=5))\n",
                "print(w2v.wv.most_similar(\"send\",topn=5))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "But it can be a more complicated query\n",
                "Word embedding spaces tend to encode much more.\n",
                "\n",
                "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_analogy(mot1, mot2, mot3, modele, top=3):\n",
                "    analogies = [(w, round(v, 2)) for w, v in  modele.most_similar(positive=[mot1,mot2], negative=[mot3], topn=top) ]    \n",
                "    return analogies   "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1 ('awful', 0.76) ('unbelievably', 0.69) ('amateur', 0.65)\n",
                        "2 ('actress', 0.9) ('role', 0.75) ('role,', 0.73)\n",
                        "3 ('roles', 0.8) ('actresses', 0.79) ('actors', 0.77)\n",
                        "4 ('woman', 0.9) ('lady', 0.84) ('girl', 0.81)\n",
                        "5 ('worst', 0.81) ('funniest', 0.7) ('greatest', 0.69)\n",
                        "6 ('funnier', 0.7) ('worse', 0.7) ('more', 0.66)\n",
                        "7 ('walking', 0.75) ('running', 0.74) ('window', 0.74)\n",
                        "8 ('sending', 0.71) ('leaving', 0.67) ('forcing', 0.66)\n",
                        "9 ('incredibly', 0.61) ('amazingly', 0.6) ('utterly', 0.6)\n"
                    ]
                }
            ],
            "source": [
                "print('1', *get_analogy(*[\"awesome\",\"bad\"], \"good\"), w2v.wv)\n",
                "print('2', *get_analogy(*[\"actor\",\"woman\"], \"man\"), w2v.wv)\n",
                "print('3', *get_analogy(*[\"actor\",\"men\"], \"man\"), w2v.wv)\n",
                "print('4', *get_analogy(*[\"man\",\"actress\"], \"actor\"), w2v.wv)\n",
                "print('5', *get_analogy(*[\"bad\",\"best\"], \"good\"), w2v.wv)\n",
                "print('6', *get_analogy(*[\"funny\",\"better\"], \"good\"), w2v.wv)\n",
                "print('7', *get_analogy(*[\"run\",\"coming\"], \"come\"), w2v.wv)\n",
                "print('8', *get_analogy(*[\"send\",\"coming\"], \"come\"), w2v.wv)\n",
                "print('9', *get_analogy(*[\"absolute\",\"totally\"], \"total\"), w2v.wv)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities.**\n",
                "\n",
                "**You can download the dataset [here](https://thome.isir.upmc.fr/classes/RITAL/questions-words.txt).**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-02-25 17:32:18,216 : INFO : Evaluating word analogies for top 300000 words in the model on datasets/questions-words.txt\n",
                        "2024-02-25 17:32:18,605 : INFO : capital-common-countries: 3.3% (3/90)\n",
                        "2024-02-25 17:32:18,903 : INFO : capital-world: 0.0% (0/71)\n",
                        "2024-02-25 17:32:19,031 : INFO : currency: 0.0% (0/28)\n",
                        "2024-02-25 17:32:20,209 : INFO : city-in-state: 0.0% (0/329)\n",
                        "2024-02-25 17:32:21,379 : INFO : family: 34.2% (117/342)\n",
                        "2024-02-25 17:32:25,494 : INFO : gram1-adjective-to-adverb: 1.4% (13/930)\n",
                        "2024-02-25 17:32:28,367 : INFO : gram2-opposite: 3.3% (18/552)\n",
                        "2024-02-25 17:32:33,195 : INFO : gram3-comparative: 24.8% (312/1260)\n",
                        "2024-02-25 17:32:35,642 : INFO : gram4-superlative: 6.3% (44/702)\n",
                        "2024-02-25 17:32:38,532 : INFO : gram5-present-participle: 15.5% (117/756)\n",
                        "2024-02-25 17:32:41,338 : INFO : gram6-nationality-adjective: 1.9% (15/792)\n",
                        "2024-02-25 17:32:45,943 : INFO : gram7-past-tense: 15.2% (191/1260)\n",
                        "2024-02-25 17:32:48,991 : INFO : gram8-plural: 4.1% (33/812)\n",
                        "2024-02-25 17:32:51,942 : INFO : gram9-plural-verbs: 19.3% (146/756)\n",
                        "2024-02-25 17:32:51,944 : INFO : Quadruplets with out-of-vocabulary words: 55.6%\n",
                        "2024-02-25 17:32:51,944 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
                        "2024-02-25 17:32:51,946 : INFO : Total accuracy: 11.6% (1009/8680)\n"
                    ]
                }
            ],
            "source": [
                "out = w2v.wv.evaluate_word_analogies(\"datasets/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well.**\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 3: Loading a pre-trained model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
                "\n",
                "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
                "\n",
                ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
                "\n",
                ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node.\n",
                "\n",
                "**You can download the pre-trained word embedding [HERE](https://thome.isir.upmc.fr/classes/RITAL/word2vec-google-news-300.dat) .**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "from gensim.test.utils import get_tmpfile\n",
                "import gensim.downloader as api\n",
                "from gensim.models import KeyedVectors\n",
                "bload = True\n",
                "fname = \"word2vec-google-news-300\"\n",
                "sdir = \"datasets/\" # Change\n",
                "\n",
                "if(bload==True):\n",
                "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
                "else:    \n",
                "    wv_pre_trained = api.load(fname)\n",
                "    wv_pre_trained.save(sdir+fname+\".dat\")\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Perform the \"synctactic\" and \"semantic\" evaluations again. Conclude on the pre-trained embeddings.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "great and good: 0.7291509\n",
                        "great and bad: 0.39287654\n"
                    ]
                }
            ],
            "source": [
                "print(\"great and good:\",wv_pre_trained.similarity(\"great\",\"good\"))\n",
                "print(\"great and bad:\",wv_pre_trained.similarity(\"great\",\"bad\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[('actress', 0.7930010557174683), ('Actor', 0.7446157932281494), ('thesp', 0.6954971551895142), ('thespian', 0.6651668548583984), ('actors', 0.6519852876663208)]\n"
                    ]
                }
            ],
            "source": [
                "print(wv_pre_trained.most_similar(\"actor\",topn=5))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[('runs', 0.656993567943573), ('running', 0.6062965393066406), ('drive', 0.4834050238132477)]\n",
                        "[('sending', 0.7407121658325195), ('sent', 0.7368507981300354), ('sends', 0.6713995337486267)]\n"
                    ]
                }
            ],
            "source": [
                "print(wv_pre_trained.most_similar(\"run\",topn=3))\n",
                "print(wv_pre_trained.most_similar(\"send\",topn=3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1 ('horrible', 0.6) ('amazing', 0.59) ('weird', 0.58)\n",
                        "2 ('actress', 0.86) ('actresses', 0.66) ('thesp', 0.63)\n",
                        "3 ('actors', 0.61) ('actresses', 0.6) ('actress', 0.54)\n",
                        "4 ('woman', 0.84) ('girl', 0.69) ('teenage_girl', 0.68)\n",
                        "5 ('worst', 0.68) ('dumbest', 0.53) ('lousiest', 0.52)\n",
                        "6 ('funnier', 0.75) ('stupider', 0.6) ('sillier', 0.57)\n",
                        "7 ('running', 0.61) ('runs', 0.5) ('Mark_Grudzielanek_singled', 0.44)\n",
                        "8 ('sending', 0.76) ('sent', 0.63) ('Sending', 0.59)\n",
                        "9 ('completely', 0.6) ('absolutely', 0.58) ('utterly', 0.55)\n"
                    ]
                }
            ],
            "source": [
                "print('1', *get_analogy(*[\"awesome\",\"bad\"], \"good\", wv_pre_trained))\n",
                "print('2', *get_analogy(*[\"actor\",\"woman\"], \"man\", wv_pre_trained))\n",
                "print('3', *get_analogy(*[\"actor\",\"men\"], \"man\", wv_pre_trained))\n",
                "print('4', *get_analogy(*[\"man\",\"actress\"], \"actor\", wv_pre_trained))\n",
                "print('5', *get_analogy(*[\"bad\",\"best\"], \"good\", wv_pre_trained))\n",
                "print('6', *get_analogy(*[\"funny\",\"better\"], \"good\", wv_pre_trained))\n",
                "print('7', *get_analogy(*[\"run\",\"coming\"], \"come\", wv_pre_trained))\n",
                "print('8', *get_analogy(*[\"send\",\"coming\"], \"come\", wv_pre_trained))\n",
                "print('9', *get_analogy(*[\"absolute\",\"totally\"], \"total\", wv_pre_trained))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## STEP 4:  sentiment classification\n",
                "\n",
                "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
                "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
                "\n",
                "\n",
                "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
                "\n",
                "\n",
                "### (1) Vectorize reviews using word vectors:\n",
                "\n",
                "Word aggregation can be done in different ways:\n",
                "\n",
                "- Sum\n",
                "- Average\n",
                "- Min/feature\n",
                "- Max/feature\n",
                "\n",
                "#### a few pointers:\n",
                "\n",
                "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
                "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 124,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We first need to vectorize text:\n",
                "# First we propose to a sum of them\n",
                "\n",
                "\n",
                "def vectorize(text, wv_model, f_aggregation, mean=False):\n",
                "    \"\"\"\n",
                "    This function should vectorize one review\n",
                "\n",
                "    input: str\n",
                "    output: np.array(float)\n",
                "    \"\"\"    \n",
                "    text_vectorized = []\n",
                "    for word in text.split():\n",
                "        # do something\n",
                "        if word in wv_model.key_to_index :\n",
                "            text_vectorized.append(wv_model[word])\n",
                "        \n",
                "    #appliquer l'aggregation par rapport aux colonnes (car vecteur lignes)\n",
                "    return f_aggregation(np.array(text_vectorized),axis=0) \n",
                "  \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 116,
            "metadata": {},
            "outputs": [],
            "source": [
                "def vectorize_all_methods(data, wv_model):\n",
                "    f_aggregation = [np.sum, np.mean, np.max, np.min]\n",
                "    train=data[:(len(data)//10)*8] # ne garder que 80% du dataset\n",
                "    test=data[len(train):]\n",
                "\n",
                "    resultats_aggregation_train = []\n",
                "    resultats_aggregation_test = []\n",
                "    y_test = [pol for text,pol in test]\n",
                "    y_train = [pol for text,pol in train]\n",
                "\n",
                "    for f in f_aggregation:\n",
                "        classes = [pol for text,pol in train]\n",
                "        X = [vectorize(text, wv_model, f) for text,pol in train]\n",
                "        X_test = [vectorize(text, wv_model, f) for text,pol in test]\n",
                "        resultats_aggregation_train.append(X)\n",
                "        resultats_aggregation_test.append(X_test)\n",
                "    \n",
                "    return resultats_aggregation_train, resultats_aggregation_test, y_train, y_test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 117,
            "metadata": {},
            "outputs": [],
            "source": [
                "text = [t.split() for t,p in data]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Vectorisation avec CBoW"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 114,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-02-25 19:37:45,593 : INFO : collecting all words and their counts\n",
                        "2024-02-25 19:37:45,596 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
                        "2024-02-25 19:37:46,597 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
                        "2024-02-25 19:37:47,554 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
                        "2024-02-25 19:37:48,057 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
                        "2024-02-25 19:37:48,058 : INFO : Creating a fresh vocabulary\n",
                        "2024-02-25 19:37:48,448 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-25T19:37:48.448641', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
                        "2024-02-25 19:37:48,448 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-25T19:37:48.448641', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
                        "2024-02-25 19:37:48,909 : INFO : deleting the raw counts dictionary of 276678 items\n",
                        "2024-02-25 19:37:48,914 : INFO : sample=0.001 downsamples 44 most-common words\n",
                        "2024-02-25 19:37:48,920 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-25T19:37:48.920045', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
                        "2024-02-25 19:37:49,630 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
                        "2024-02-25 19:37:49,632 : INFO : resetting layer weights\n",
                        "2024-02-25 19:37:49,673 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-25T19:37:49.673861', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
                        "2024-02-25 19:37:49,675 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-25T19:37:49.675353', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
                        "2024-02-25 19:37:50,682 : INFO : EPOCH 0 - PROGRESS: at 17.84% examples, 738986 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:37:51,691 : INFO : EPOCH 0 - PROGRESS: at 36.24% examples, 751768 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:37:52,694 : INFO : EPOCH 0 - PROGRESS: at 54.22% examples, 745710 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:37:53,694 : INFO : EPOCH 0 - PROGRESS: at 76.39% examples, 789345 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:37:54,702 : INFO : EPOCH 0 - PROGRESS: at 99.03% examples, 820811 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:37:54,747 : INFO : EPOCH 0: training on 5713167 raw words (4165769 effective words) took 5.1s, 822075 effective words/s\n",
                        "2024-02-25 19:37:55,754 : INFO : EPOCH 1 - PROGRESS: at 23.61% examples, 979992 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:37:56,751 : INFO : EPOCH 1 - PROGRESS: at 46.85% examples, 969715 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:37:57,761 : INFO : EPOCH 1 - PROGRESS: at 69.78% examples, 964528 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:37:58,764 : INFO : EPOCH 1 - PROGRESS: at 91.86% examples, 951886 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:37:59,099 : INFO : EPOCH 1: training on 5713167 raw words (4165389 effective words) took 4.4s, 957189 effective words/s\n",
                        "2024-02-25 19:38:00,112 : INFO : EPOCH 2 - PROGRESS: at 23.61% examples, 980973 words/s, in_qsize 4, out_qsize 1\n",
                        "2024-02-25 19:38:01,117 : INFO : EPOCH 2 - PROGRESS: at 44.24% examples, 916883 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:38:02,118 : INFO : EPOCH 2 - PROGRESS: at 63.53% examples, 877707 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:38:03,126 : INFO : EPOCH 2 - PROGRESS: at 84.29% examples, 871877 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:38:03,957 : INFO : EPOCH 2: training on 5713167 raw words (4165927 effective words) took 4.8s, 859098 effective words/s\n",
                        "2024-02-25 19:38:04,982 : INFO : EPOCH 3 - PROGRESS: at 21.88% examples, 892028 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:38:05,995 : INFO : EPOCH 3 - PROGRESS: at 45.38% examples, 930328 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:38:07,003 : INFO : EPOCH 3 - PROGRESS: at 68.02% examples, 930747 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:38:08,001 : INFO : EPOCH 3 - PROGRESS: at 88.71% examples, 910967 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:38:08,464 : INFO : EPOCH 3: training on 5713167 raw words (4164343 effective words) took 4.5s, 924517 effective words/s\n",
                        "2024-02-25 19:38:09,478 : INFO : EPOCH 4 - PROGRESS: at 23.46% examples, 969441 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:38:10,481 : INFO : EPOCH 4 - PROGRESS: at 47.87% examples, 991075 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:38:11,472 : INFO : EPOCH 4 - PROGRESS: at 70.78% examples, 978972 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:38:12,478 : INFO : EPOCH 4 - PROGRESS: at 95.50% examples, 989548 words/s, in_qsize 4, out_qsize 1\n",
                        "2024-02-25 19:38:12,662 : INFO : EPOCH 4: training on 5713167 raw words (4164915 effective words) took 4.2s, 993108 effective words/s\n",
                        "2024-02-25 19:38:12,662 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20826343 effective words) took 23.0s, 905907 effective words/s', 'datetime': '2024-02-25T19:38:12.662172', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
                        "2024-02-25 19:38:12,666 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-25T19:38:12.666866', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n"
                    ]
                }
            ],
            "source": [
                "# the following configuration is the default configuration\n",
                "w2v_cbow = gensim.models.word2vec.Word2Vec(sentences=text,\n",
                "                                vector_size=100, window=5,               ### here we train a cbow model \n",
                "                                min_count=5,                      \n",
                "                                sample=0.001, workers=3,\n",
                "                                sg=0, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
                "                                cbow_mean=1, epochs=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 118,
            "metadata": {},
            "outputs": [],
            "source": [
                "resultats_aggregation_train_cbow, resultats_aggregation_test_cbow, y_train, y_test = vectorize_all_methods(data, w2v_cbow.wv)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Vectorisation SkipGram"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 120,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-02-25 19:43:47,668 : INFO : collecting all words and their counts\n",
                        "2024-02-25 19:43:47,671 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
                        "2024-02-25 19:43:48,813 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
                        "2024-02-25 19:43:49,876 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
                        "2024-02-25 19:43:50,337 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
                        "2024-02-25 19:43:50,337 : INFO : Creating a fresh vocabulary\n",
                        "2024-02-25 19:43:50,721 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-25T19:43:50.721517', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
                        "2024-02-25 19:43:50,721 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-25T19:43:50.721517', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
                        "2024-02-25 19:43:51,180 : INFO : deleting the raw counts dictionary of 276678 items\n",
                        "2024-02-25 19:43:51,189 : INFO : sample=0.001 downsamples 44 most-common words\n",
                        "2024-02-25 19:43:51,189 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-25T19:43:51.189510', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'prepare_vocab'}\n",
                        "2024-02-25 19:43:51,906 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
                        "2024-02-25 19:43:51,909 : INFO : resetting layer weights\n",
                        "2024-02-25 19:43:51,945 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-25T19:43:51.945863', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'build_vocab'}\n",
                        "2024-02-25 19:43:51,945 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-25T19:43:51.945863', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
                        "2024-02-25 19:43:52,968 : INFO : EPOCH 0 - PROGRESS: at 4.73% examples, 197609 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:43:53,995 : INFO : EPOCH 0 - PROGRESS: at 9.84% examples, 203150 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:43:55,015 : INFO : EPOCH 0 - PROGRESS: at 16.82% examples, 228292 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:43:56,028 : INFO : EPOCH 0 - PROGRESS: at 23.09% examples, 236364 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:43:57,029 : INFO : EPOCH 0 - PROGRESS: at 30.01% examples, 245739 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:43:58,053 : INFO : EPOCH 0 - PROGRESS: at 36.85% examples, 252720 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:43:59,063 : INFO : EPOCH 0 - PROGRESS: at 43.90% examples, 257254 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:00,088 : INFO : EPOCH 0 - PROGRESS: at 50.84% examples, 259838 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:01,122 : INFO : EPOCH 0 - PROGRESS: at 58.19% examples, 263182 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:02,161 : INFO : EPOCH 0 - PROGRESS: at 65.24% examples, 265826 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:03,163 : INFO : EPOCH 0 - PROGRESS: at 71.67% examples, 265887 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:04,223 : INFO : EPOCH 0 - PROGRESS: at 78.58% examples, 265589 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:05,250 : INFO : EPOCH 0 - PROGRESS: at 85.16% examples, 266208 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:06,256 : INFO : EPOCH 0 - PROGRESS: at 91.73% examples, 266596 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:07,264 : INFO : EPOCH 0 - PROGRESS: at 98.55% examples, 267882 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:07,474 : INFO : EPOCH 0: training on 5713167 raw words (4166153 effective words) took 15.5s, 268408 effective words/s\n",
                        "2024-02-25 19:44:08,486 : INFO : EPOCH 1 - PROGRESS: at 6.25% examples, 262544 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:09,505 : INFO : EPOCH 1 - PROGRESS: at 13.38% examples, 274523 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:10,549 : INFO : EPOCH 1 - PROGRESS: at 20.21% examples, 274314 words/s, in_qsize 4, out_qsize 1\n",
                        "2024-02-25 19:44:11,561 : INFO : EPOCH 1 - PROGRESS: at 26.86% examples, 274103 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:12,607 : INFO : EPOCH 1 - PROGRESS: at 33.60% examples, 272734 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:13,620 : INFO : EPOCH 1 - PROGRESS: at 40.94% examples, 277366 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:14,650 : INFO : EPOCH 1 - PROGRESS: at 48.58% examples, 281395 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:15,671 : INFO : EPOCH 1 - PROGRESS: at 55.73% examples, 282139 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:16,714 : INFO : EPOCH 1 - PROGRESS: at 62.83% examples, 283148 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:17,772 : INFO : EPOCH 1 - PROGRESS: at 69.60% examples, 281402 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:18,789 : INFO : EPOCH 1 - PROGRESS: at 76.55% examples, 280995 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:19,819 : INFO : EPOCH 1 - PROGRESS: at 83.52% examples, 280626 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:20,840 : INFO : EPOCH 1 - PROGRESS: at 89.42% examples, 277721 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:21,875 : INFO : EPOCH 1 - PROGRESS: at 95.82% examples, 276665 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:22,502 : INFO : EPOCH 1: training on 5713167 raw words (4163598 effective words) took 15.0s, 276938 effective words/s\n",
                        "2024-02-25 19:44:23,522 : INFO : EPOCH 2 - PROGRESS: at 6.42% examples, 269923 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:24,540 : INFO : EPOCH 2 - PROGRESS: at 13.38% examples, 275970 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:25,556 : INFO : EPOCH 2 - PROGRESS: at 20.25% examples, 277080 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:26,596 : INFO : EPOCH 2 - PROGRESS: at 27.37% examples, 280065 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:27,602 : INFO : EPOCH 2 - PROGRESS: at 33.74% examples, 276715 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:28,618 : INFO : EPOCH 2 - PROGRESS: at 39.83% examples, 272592 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:29,615 : INFO : EPOCH 2 - PROGRESS: at 46.46% examples, 272464 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:30,630 : INFO : EPOCH 2 - PROGRESS: at 53.51% examples, 273632 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:31,636 : INFO : EPOCH 2 - PROGRESS: at 60.51% examples, 275717 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:32,643 : INFO : EPOCH 2 - PROGRESS: at 66.90% examples, 275400 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:33,665 : INFO : EPOCH 2 - PROGRESS: at 74.16% examples, 276527 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:34,710 : INFO : EPOCH 2 - PROGRESS: at 80.99% examples, 275497 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:35,709 : INFO : EPOCH 2 - PROGRESS: at 87.87% examples, 276398 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:36,718 : INFO : EPOCH 2 - PROGRESS: at 93.66% examples, 274492 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:37,692 : INFO : EPOCH 2: training on 5713167 raw words (4165152 effective words) took 15.2s, 274544 effective words/s\n",
                        "2024-02-25 19:44:38,720 : INFO : EPOCH 3 - PROGRESS: at 6.80% examples, 278193 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:39,726 : INFO : EPOCH 3 - PROGRESS: at 14.06% examples, 288084 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:40,752 : INFO : EPOCH 3 - PROGRESS: at 20.91% examples, 284048 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:41,873 : INFO : EPOCH 3 - PROGRESS: at 27.21% examples, 271247 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:42,883 : INFO : EPOCH 3 - PROGRESS: at 33.91% examples, 272291 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:43,940 : INFO : EPOCH 3 - PROGRESS: at 40.94% examples, 273087 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:44,952 : INFO : EPOCH 3 - PROGRESS: at 48.38% examples, 277280 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:45,963 : INFO : EPOCH 3 - PROGRESS: at 55.73% examples, 279658 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:46,963 : INFO : EPOCH 3 - PROGRESS: at 62.66% examples, 281192 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:47,982 : INFO : EPOCH 3 - PROGRESS: at 69.94% examples, 282738 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:48,997 : INFO : EPOCH 3 - PROGRESS: at 77.33% examples, 283620 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:50,008 : INFO : EPOCH 3 - PROGRESS: at 83.69% examples, 282085 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:51,038 : INFO : EPOCH 3 - PROGRESS: at 90.44% examples, 281287 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:44:52,049 : INFO : EPOCH 3 - PROGRESS: at 96.70% examples, 280074 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:52,592 : INFO : EPOCH 3: training on 5713167 raw words (4164545 effective words) took 14.9s, 279548 effective words/s\n",
                        "2024-02-25 19:44:53,603 : INFO : EPOCH 4 - PROGRESS: at 5.90% examples, 248428 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:54,612 : INFO : EPOCH 4 - PROGRESS: at 13.05% examples, 269311 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:55,630 : INFO : EPOCH 4 - PROGRESS: at 19.52% examples, 268011 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:56,630 : INFO : EPOCH 4 - PROGRESS: at 26.20% examples, 270565 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:57,622 : INFO : EPOCH 4 - PROGRESS: at 33.05% examples, 273502 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:58,632 : INFO : EPOCH 4 - PROGRESS: at 40.20% examples, 277470 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:44:59,641 : INFO : EPOCH 4 - PROGRESS: at 46.66% examples, 275475 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:45:00,671 : INFO : EPOCH 4 - PROGRESS: at 53.88% examples, 276672 words/s, in_qsize 6, out_qsize 0\n",
                        "2024-02-25 19:45:01,711 : INFO : EPOCH 4 - PROGRESS: at 60.96% examples, 278119 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:45:02,717 : INFO : EPOCH 4 - PROGRESS: at 67.98% examples, 279501 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:45:03,771 : INFO : EPOCH 4 - PROGRESS: at 75.50% examples, 280520 words/s, in_qsize 4, out_qsize 1\n",
                        "2024-02-25 19:45:04,765 : INFO : EPOCH 4 - PROGRESS: at 82.83% examples, 282278 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:45:05,776 : INFO : EPOCH 4 - PROGRESS: at 89.98% examples, 283135 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:45:06,808 : INFO : EPOCH 4 - PROGRESS: at 96.99% examples, 283876 words/s, in_qsize 5, out_qsize 0\n",
                        "2024-02-25 19:45:07,237 : INFO : EPOCH 4: training on 5713167 raw words (4165132 effective words) took 14.6s, 284503 effective words/s\n",
                        "2024-02-25 19:45:07,237 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20824580 effective words) took 75.3s, 276606 effective words/s', 'datetime': '2024-02-25T19:45:07.237540', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'train'}\n",
                        "2024-02-25 19:45:07,237 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-25T19:45:07.237540', 'gensim': '4.3.0', 'python': '3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22631-SP0', 'event': 'created'}\n"
                    ]
                }
            ],
            "source": [
                "# the following configuration is the default configuration\n",
                "w2v_sg = gensim.models.word2vec.Word2Vec(sentences=text,\n",
                "                                vector_size=100, window=5,               ### here we train a cbow model \n",
                "                                min_count=5,                      \n",
                "                                sample=0.001, workers=3,\n",
                "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
                "                                epochs=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 121,
            "metadata": {},
            "outputs": [],
            "source": [
                "resultats_aggregation_train_sg, resultats_aggregation_test_sg, y_train, y_test = vectorize_all_methods(data, w2v_sg.wv)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Vectorisation wv_pretrained"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 125,
            "metadata": {},
            "outputs": [],
            "source": [
                "resultats_aggregation_train_pre_trained, resultats_aggregation_test_pre_trained, y_train, y_test = vectorize_all_methods(data, wv_pre_trained)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### (2) Train a classifier \n",
                "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Classification avec cbow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 131,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy= 0.64\n"
                    ]
                }
            ],
            "source": [
                "methodes_aggregation = {\"np.sum\":0, \"np.mean\": 1, \"np.max\": 2, \"np.min\": 3}\n",
                "i = methodes_aggregation[\"np.sum\"]\n",
                "\n",
                "X_train_cbow = pd.DataFrame(resultats_aggregation_train_cbow[i])\n",
                "X_test_cbow = pd.DataFrame(resultats_aggregation_test_cbow[i])\n",
                "y_train = pd.DataFrame(y_train).iloc[:,0]\n",
                "y_test = pd.DataFrame(y_test).iloc[:,0]\n",
                "\n",
                "model_lr = LogisticRegression()\n",
                "model_lr.fit(X_train_cbow, y_train)\n",
                "y_pred_cbow = model_lr.predict(X_test_cbow)\n",
                "accuracy_cbow= accuracy_score(y_test,y_pred_cbow)\n",
                "print('Accuracy=', accuracy_cbow)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Classification avec SkipGram"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 136,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy= 0.7406\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "methodes_aggregation = {\"np.sum\":0, \"np.mean\": 1, \"np.max\": 2, \"np.min\": 3}\n",
                "i = methodes_aggregation[\"np.sum\"]\n",
                "\n",
                "X_train_sg = pd.DataFrame(resultats_aggregation_train_sg[i])\n",
                "X_test_sg = pd.DataFrame(resultats_aggregation_test_sg[i])\n",
                "y_train = pd.DataFrame(y_train).iloc[:,0]\n",
                "y_test = pd.DataFrame(y_test).iloc[:,0]\n",
                "\n",
                "model_lr = LogisticRegression()\n",
                "model_lr.fit(X_train_sg, y_train)\n",
                "y_pred_sg = model_lr.predict(X_test_sg)\n",
                "accuracy_sg = accuracy_score(y_test,y_pred_sg)\n",
                "print('Accuracy=', accuracy_sg)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Classification avec wv_pre_trained"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 137,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy= 0.7604\n"
                    ]
                }
            ],
            "source": [
                "methodes_aggregation = {\"np.sum\":0, \"np.mean\": 1, \"np.max\": 2, \"np.min\": 3}\n",
                "i = methodes_aggregation[\"np.sum\"]\n",
                "\n",
                "X_train_pre_trained = pd.DataFrame(resultats_aggregation_train_pre_trained[i])\n",
                "X_test_pre_trained = pd.DataFrame(resultats_aggregation_test_pre_trained[i])\n",
                "y_train = pd.DataFrame(y_train).iloc[:,0]\n",
                "y_test = pd.DataFrame(y_test).iloc[:,0]\n",
                "\n",
                "model_lr = LogisticRegression()\n",
                "model_lr.fit(X_train_pre_trained, y_train)\n",
                "y_pred_pre_trained = model_lr.predict(X_test_pre_trained)\n",
                "accuracy_pre_trained = accuracy_score(y_test,y_pred_pre_trained)\n",
                "print('Accuracy=', accuracy_pre_trained)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
                "\n",
                "## **Todo** :  Try answering the following questions:\n",
                "\n",
                "- Which word2vec model works best: skip-gram or cbow\n",
                "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "- SkipGram fonctionne mieux que Cbow (74 > 67) <br>\n",
                "- Oui, les modèles pré-entrainés fonctionnent mieux que les autres car ils ont été entraînés sur des corpus plus larges et ils peuvent donc mieux capturer les dépendances entre les mots/phrases "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "**(Bonus)** To have a better accuracy, we could try two things:\n",
                "- Better aggregation methods (weight by tf-idf ?)\n",
                "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
                "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        },
        "vscode": {
            "interpreter": {
                "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
